{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Classifier Copies - Online Copies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes with complex feature spaces, with many dimensions, or with complex original classifiers, the volume of generated synthetic data needed to carry out the classifier copy is too large to fit in the computer memory at the same time. In these cases we can resort to **online copying**.\n",
    "\n",
    "We have implemented a version that works with standalone sklearn classifiers as well as pipelines, as long as each transformer and estimator has a *partial_fit* function implemented. There is more than one way to train pipelines with incremental transformers and estimators, here we simply take each data batch and train the first element of the pipeline, then transform the incoming data with the first element, then train the second, then transform the data with the second, use this transformed data to train the third, etc.\n",
    "\n",
    "The implementation has two gears that can function independently: \n",
    "* *SyntheticDataStreamer()*: a continous synthetic data generator.\n",
    "* *ContinuousCopy()*: a continuous ML classifier copier.\n",
    "\n",
    "These two elements can share a queue, where the instance of the first class can add data whenever there's an empy slot, and from which the instance of the second class can take a batch of data, as soon as there is any available.\n",
    "\n",
    "The classifier copy can be trained for an arbitrary amount of time and then saved, to continue at a later time with more training.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from queue import Queue\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from presc.dataset import Dataset\n",
    "from presc.copies.continuous import SyntheticDataStreamer, ContinuousCopy\n",
    "from presc.copies.copying import ClassifierCopy\n",
    "from presc.copies.sampling import normal_sampling\n",
    "\n",
    "from ML_copies_original_models import SegmentationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load \"black box\" model to copy\n",
    "\n",
    "For this example, we take an existing \"black box\" classifier model example of the publicly available [Image Segmentation dataset](https://archive-beta.ics.uci.edu/ml/datasets/Image+Segmentation), that we can query in order to obtain a copy. This problem has 7 classes: \n",
    "\n",
    "    BRICKFACE, CEMENT, FOLIAGE, GRASS, PATH, SKY, and WINDOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Load problem\n",
    "original_model = SegmentationModel()\n",
    "\n",
    "# Description of the feature space of the problem to carry out the sampling\n",
    "feature_description = original_model.feature_description\n",
    "\n",
    "# Original test data \n",
    "test_data = Dataset(original_model.X_test.join(original_model.y_test), label_col=\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Classifier Copy\n",
    "\n",
    "Whatever transformer or estimator family that we use for the copy needs to have an implementatin of a *partial_fit* function to be copied incrementally. Below there's a list of some of the possible transformers and classifiers implemented in sci-kit learn that we can use on their own or within a pipeline.\n",
    "\n",
    "#### Possible sklearn incremental learning preprocessing:\n",
    "* sklearn.preprocessing.StandardScaler\n",
    "* sklearn.preprocessing.MinMaxScaler\n",
    "* sklearn.preprocessing.MaxAbsScaler\n",
    "\n",
    "#### Possible sklearn incremental learning classifiers:\n",
    "* sklearn.naive_bayes.MultinomialNB\n",
    "* sklearn.naive_bayes.BernoulliNB\n",
    "* sklearn.linear_model.Perceptron\n",
    "* sklearn.linear_model.SGDClassifier\n",
    "* sklearn.linear_model.PassiveAggressiveClassifier\n",
    "* sklearn.neural_network.MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the copy pipepline\n",
    "sdg_normal_classifier = Pipeline([('scaler', StandardScaler()), ('sdg_classifier', \n",
    "                                   SGDClassifier())])\n",
    "\n",
    "# Define the parameters for the copying balancer (which ensures equal amount of samples from each class)\n",
    "balance_parameters={\"max_iter\": 50, \"nbatch\": 10000, \"verbose\": False}\n",
    "\n",
    "# Instantiate the copier class\n",
    "sdg_normal_copy = ClassifierCopy(original_model.model, sdg_normal_classifier, normal_sampling,\n",
    "                                  enforce_balance=False, nsamples=20000, random_state=42,\n",
    "                                  feature_parameters=feature_description, label_col=\"class\",\n",
    "                                  **balance_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate shared queue\n",
    "\n",
    "Here we instantiate queue that will be shared between the synthetic data generator and the copying class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stream = Queue(maxsize=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate and start data streamer\n",
    "\n",
    "The classifier copier instance and the shared queue are the necessary parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_streamer = SyntheticDataStreamer(sdg_normal_copy, data_stream, verbose=True)\n",
    "data_streamer.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate and start classifier copier\n",
    "\n",
    "We can specify any parameters we need for the *partial_fit* with the *fit_kwargs* parameter. For a single classifier we simply add them in a dictionary. If using a pipeline, a dictionary with an entry for each element used in the pipeline is necessary, and then with each entry containing the parameter dictionary for that transformer or estimator.\n",
    "\n",
    "In this example we use the SDGClassifier, for which classes need to be specified.\n",
    "\n",
    "If we want the evaluation summary of the classifier to print after each iteration, we set **verbose=True** when instantiating the online copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  1\n",
      "Samples:  20000 \n",
      "\n",
      "Original Model Accuracy (test)          0.9500\n",
      "Copy Model Accuracy (test)              0.6952\n",
      "Empirical Fidelity Error (synthetic)    0.3502\n",
      "Empirical Fidelity Error (test)         0.2833\n",
      "Replacement Capability (synthetic)      0.6498\n",
      "Replacement Capability (test)           0.7318\n",
      "\n",
      "Iteration:  2\n",
      "Samples:  40000 \n",
      "\n",
      "Original Model Accuracy (test)          0.9500\n",
      "Copy Model Accuracy (test)              0.7381\n",
      "Empirical Fidelity Error (synthetic)    0.3351\n",
      "Empirical Fidelity Error (test)         0.2405\n",
      "Replacement Capability (synthetic)      0.6649\n",
      "Replacement Capability (test)           0.7769\n",
      "\n",
      "Iteration:  3\n",
      "Samples:  60000 \n",
      "\n",
      "Original Model Accuracy (test)          0.9500\n",
      "Copy Model Accuracy (test)              0.5845\n",
      "Empirical Fidelity Error (synthetic)    0.3615\n",
      "Empirical Fidelity Error (test)         0.4095\n",
      "Replacement Capability (synthetic)      0.6385\n",
      "Replacement Capability (test)           0.6153\n",
      "\n",
      "Iteration:  4\n",
      "Samples:  80000 \n",
      "\n",
      "Original Model Accuracy (test)          0.9500\n",
      "Copy Model Accuracy (test)              0.6929\n",
      "Empirical Fidelity Error (synthetic)    0.3053\n",
      "Empirical Fidelity Error (test)         0.2833\n",
      "Replacement Capability (synthetic)      0.6947\n",
      "Replacement Capability (test)           0.7293\n",
      "\n",
      "Iteration:  5\n",
      "Samples:  100000 \n",
      "\n",
      "Original Model Accuracy (test)          0.9500\n",
      "Copy Model Accuracy (test)              0.7012\n",
      "Empirical Fidelity Error (synthetic)    0.3376\n",
      "Empirical Fidelity Error (test)         0.2774\n",
      "Replacement Capability (synthetic)      0.6624\n",
      "Replacement Capability (test)           0.7381\n",
      "\n",
      "Iteration:  6\n",
      "Samples:  120000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specific parameters needed to fit each element of the pipeline\n",
    "fit_kwargs = {\"scaler\": {}, \"sdg_classifier\": {\"classes\": ['BRICKFACE', 'CEMENT', 'FOLIAGE', \n",
    "                                                           'GRASS', 'PATH', 'SKY', 'WINDOW']}}\n",
    "\n",
    "# Instantiate and start copy\n",
    "online_copy = ContinuousCopy(sdg_normal_copy, data_stream, fit_kwargs=fit_kwargs, \n",
    "                             verbose=True, test_data=test_data)\n",
    "online_copy.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop data streamer and online copy\n",
    "\n",
    "Once you are finished carrying out the classifier copy, make sure to stop both the data streamer and the online copy instances, or they may stay in the background using resources from your computer.\n",
    "\n",
    "If you stop the data streamer before stopping the online copier, the copier may keep going for a few iterations until there are't any data batches left in the queue, so it may be better to stop the copier first.\n",
    "\n",
    "Sometimes it is not immediate and it takes a while for the threads to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online copier is running: True\n",
      "\n",
      "Stopping online classifier copier...\n",
      "\n",
      "The classifier copy trained for 6 iterations\n",
      "with a total of 120.000 samples.\n",
      "\n",
      "Original Model Accuracy (test)          0.9500\n",
      "Copy Model Accuracy (test)              0.6274\n",
      "Empirical Fidelity Error (synthetic)    0.3261\n",
      "Empirical Fidelity Error (test)         0.3464\n",
      "Replacement Capability (synthetic)      0.6739\n",
      "Replacement Capability (test)           0.6604\n"
     ]
    }
   ],
   "source": [
    "# Check if the online copier is still running\n",
    "print(f\"Online copier is running: {online_copy.is_alive()}\\n\")\n",
    "\n",
    "# Stop the online copier\n",
    "online_copy.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data streamer is running: True\n",
      "\n",
      "Stopping data streamer...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if the data streamer is still running\n",
    "print(f\"Data streamer is running: {data_streamer.is_alive()}\\n\")\n",
    "\n",
    "# Stop the thread\n",
    "data_streamer.stop()\n",
    "_ = data_stream.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data streamer is running: False\n",
      "Online copier is running: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if any of the two threads are still running\n",
    "print(f\"Data streamer is running: {data_streamer.is_alive()}\")\n",
    "print(f\"Online copier is running: {online_copy.is_alive()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
